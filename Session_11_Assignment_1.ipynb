{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Questions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:black; font-size:1.1em;\">**Q 1. What are the three stages to build the hypotheses or model in machine learning?**</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 1) Data preparation: Here data is understood very well and a relationship drawn between variables. If required, transformation happens on requried data. \n",
    "- 2) Model development: Here a Model or appropriate algorithm is trained on your data. Basically, a relationship is learnt by the model and it draws a formula with required coefficients. \n",
    "- 3) Model validation/deployment/monitoring: Although model development involves model validation as well where a good fit model is promoted to production. However, model is also monitored on the production on required interval so that unseen variance in data could push model to perform poor. In that case a model could be redeveloped to adopt such variance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:black; font-size:1.1em;\">**Q 2. What is the standard approach to supervised learning?**</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In supervised learning, after data cleaning and visualization the data with important variables are splitted into two or three quantities. Called Train, validate and test or sometimes only train and validate. Then a relationship between dependant variable and independant variable is observed through graphs or .corr function. Only positive or negative variables with strong relationship are considered for the model. A model is trained by providing a dependant variable and significant independant variables. Model develops a relation or provides a intercept and coefficients which will be used to predict the future values of dependant variable. Using same model, a model is validated by passing a validation or test set of data. If model score is consistent and good in both training and validation then its a Good Fit model. If model has poor score in training then its an underfit model so it means there are not enough information in the data. If model is having good score at training but poor at test then it means its overfit. In case of overfit, model is reconsidered for training to generalize it well. There are many post validation steps. Where model assumptions are validated using statistics or graphs.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:black; font-size:1.1em;\">**Q 3. What is Training set and Test set?**</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training data set is roughly 60 or 70% of actual sample data. This data is used to train the model and learn a relationship between dependant and independant variables. Test dataset is typically 30-40% of actual dataset where it is used to validate or test a model for evaluating how well model is. Is it good or under or over fit. Training and test dataset are kept different so that variance in a test data kept away from model while training. So in future on production environment, if there are variances or unknown pattern in the data then model will be strong enough to predict with better accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:black; font-size:1.1em;\">**Q 4. What is the general principle of an ensemble method and what is bagging and\n",
    "boosting in ensemble method?**</span>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "General purpose of ensemble is to develop multiple models either of same type or different types either using same or different variables. If any single ML model not consistent enough to predict with good accuracy then ensemble method is used to average out the predicted output by multiple models.\n",
    "\n",
    "Bagging ensemble method is a method in which multiple models are bagged together and aggregated prediction by those models used as final output. This gives more stability than a single ML model. Here all the models runs parallely.\n",
    "\n",
    "Boosting ensemble method is a method in which multiple models are built in a sequential pipeline. So whichever prediction has poor accuracy or high misclassification rate, they goes through another chain of model until an acceptable prediction occurs. So purpose of sequential model is to boost the accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:black; font-size:1.1em;\">**Q 5. How can you avoid overfitting ?**</span>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Typically model is overfit when mode learns outliers or insignificant information during training. In overfit model, model performs very well in training but performs poor in testing. To avoid overfit, model has to be generalized so that it is not biased by any outliers or insignificant information during model training. Thus the model should perform well with any unseen data and provide good accuracy like it is giving with train data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
